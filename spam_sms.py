# -*- coding: utf-8 -*-
"""SPAM SMS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AtmtoAV6JcyMkiloYNgSlkKcPSV31vH8
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

df = pd.read_csv("/content/spam.csv",encoding='ISO-8859-1')
df.head()

df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
# Rename columns for better clarity
df.columns = ['Label', 'Message']
df.head()

import matplotlib.pyplot as plt
# Count duplicated rows
duplicate_count = df.duplicated().sum()

print(f"Number of duplicate rows: {duplicate_count}")

# Prepare data
unique_count = len(df) - duplicate_count
row_types = ['Unique Rows', 'Duplicate Rows']
counts = [unique_count, duplicate_count]

plt.figure(figsize=(8, 6))
plt.pie(
    counts,
    labels=row_types,
    autopct='%1.1f%%',
    startangle=140,
    colors=["#FFA07A", "#D3D3D3"],
    explode=(0.05, 0.15),
    textprops={'fontsize': 14, 'color': '#000000', 'fontweight': 'bold'}
)
plt.title('Duplicate Rows Distribution', fontsize=18, color='#000000', fontweight='bold')
plt.show()

# Show duplicate rows
duplicates = df[df.duplicated()]
print(duplicates.head())
# Check the label distribution among duplicates
print("\n----To check label distribution on duplicate----")
print(duplicates['Label'].value_counts())

df=df.drop_duplicates()
print("remaining data shape: ",df.shape)

# Count the number of ham and spam messages
label_counts = df['Label'].value_counts()

# Plot the pie chart
plt.figure(figsize=(8, 6))
plt.pie(
    label_counts,
    labels=['Ham', 'Spam'],
    autopct='%1.1f%%',
    startangle=140,
    colors=["#FFA07A", "#D3D3D3"],
    explode=(0.05, 0.15),
    textprops={'fontsize': 14, 'color': '#000000', 'fontweight': 'bold'}
)
plt.title('Proportion of Ham vs Spam', fontsize=18, color='#000000', fontweight='bold')
plt.show()

from wordcloud import WordCloud

# Word Cloud for Ham
ham_words = ' '.join(df[df['Label'] == 'ham']['Message'])
ham_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='autumn').generate(ham_words)

# Word Cloud for Spam
spam_words = ' '.join(df[df['Label'] == 'spam']['Message'])
spam_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Oranges').generate(spam_words)

# Display word clouds
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Ham Messages', fontsize=16, color='#FF6600', fontweight='bold')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Spam Messages', fontsize=16, color='#FF6600', fontweight='bold')
plt.axis('off')

plt.show()

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')  # Optional, to ensure full support for lemmatization

import nltk
nltk.download('stopwords') # Download the stopwords dataset

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# Initialize Stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))


# Custom abbreviation mapping
abbreviations = {
    "u": "you",
    "ur": "your",
    "la": "the",  # Optional: Add abbreviation mappings here
    "wat": "what",
    "n": "and",
    "e": "the"
}

# Function to handle text normalization with stemming, stopwords, and abbreviation handling
def normalize_text(text):
    """
    Simplified text normalization: lowercasing, punctuation removal, stopword removal,
    abbreviation handling, and stemming.
    """
    # Convert text to lowercase
    text = text.lower()

    # Tokenize the text into words
    words = word_tokenize(text)
    # Expand abbreviations
    words = [abbreviations.get(word, word) for word in words]

    # Remove punctuation, stopwords, and non-alphabetic words
    words = [re.sub(r"[^\w\s]", "", word) for word in words if word.isalpha() and word not in stop_words]

    # Stem words
    words = [stemmer.stem(word) for word in words]

    # Join words back into a single string
    normalized_text = " ".join(words)
    return normalized_text

import nltk

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# Initialize Stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))


# Custom abbreviation mapping
abbreviations = {
    "u": "you",
    "ur": "your",
    "la": "the",  # Optional: Add abbreviation mappings here
    "wat": "what",
    "n": "and",
    "e": "the"
}

# Function to handle text normalization with stemming, stopwords, and abbreviation handling
def normalize_text(text):
    """
    Simplified text normalization: lowercasing, punctuation removal, stopword removal,
    abbreviation handling, and stemming.
    """
    # Convert text to lowercase
    text = text.lower()

    # Tokenize the text into words
    words = word_tokenize(text)
    # Expand abbreviations
    words = [abbreviations.get(word, word) for word in words]

    # Remove punctuation, stopwords, and non-alphabetic words
    words = [re.sub(r"[^\w\s]", "", word) for word in words if word.isalpha() and word not in stop_words]

    # Stem words
    words = [stemmer.stem(word) for word in words]

    # Join words back into a single string
    normalized_text = " ".join(words)
    return normalized_text

# Apply the normalize_text function to the 'Message' column
df['Normalized_Message'] = df['Message'].apply(normalize_text)

df.head()

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
import pandas as pd
from imblearn.over_sampling import SMOTE

# Assuming df is your dataframe with 'text' and 'label' columns

# 1. TF-IDF Vectorization
# Split features and labels
X = df['Message']
y = df['Label']

# Convert text data into numerical form using TfidfVectorizer
vectorizer = TfidfVectorizer()
X_transformed = vectorizer.fit_transform(X)

feature_names = vectorizer.get_feature_names_out()
print(f"Number of features after TF-IDF: {len(feature_names)}")

# 2. Variance Threshold Feature Selection
selector = VarianceThreshold(threshold=1e-4) # No feature in X meets the variance threshold 0.10
X_selected = selector.fit_transform(X_transformed.toarray())

# 3. Report removed features
features_kept = selector.get_support()
n_removed = len(features_kept) - sum(features_kept)
print(f"\nFeatures removed: {n_removed}")
print(f"Features remaining: {sum(features_kept)}")
print(f"Total remained by (%): {sum(features_kept)/len(features_kept)*100}")

# 4. Stratified Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_selected,
    y,
    test_size=0.3,
    shuffle=False,
    random_state=1234,
    stratify=None  # No shuffle means no stratify
)

# 5. Report matrix shapes
print(f"\nTraining set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

# 6. Report top and bottom 10 rows
print("\nTop 10 rows of training set:")
print(X_train[:10])
print("\nBottom 10 rows of training set:")
print(X_train[-10:])

# 7. Naive Bayes Classification
nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"\nAccuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)

for var_ in [1e-3,1e-4,1e-5,1e-6]:
    print(f"\n----------- Result at variance threshold = {var_} ------------\n")
    # Variance Threshold Feature Selection
    selector = VarianceThreshold(threshold=var_) # No feature in X meets the variance threshold 0.10
    X_selected = selector.fit_transform(X_transformed.toarray())
    # Report removed features
    features_kept = selector.get_support()
    n_removed = len(features_kept) - sum(features_kept)
    print(f"\nFeatures removed: {n_removed}")
    print(f"Features remaining: {sum(features_kept)}")
    print(f"Total remained by (%): {sum(features_kept)/len(features_kept)*100}")
    # Stratified Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected,
        y,
        test_size=0.3,
        shuffle=False,
        random_state=1234,
        stratify=None  # No shuffle means no stratify
    )
     # Report matrix shapes
    print(f"\nTraining set shape: {X_train.shape}")
    print(f"Testing set shape: {X_test.shape}")
    # Report top and bottom 10 rows
    print("\nTop 10 rows of training set:")
    print(X_train[:10])
    print("\nBottom 10 rows of training set:")
    print(X_train[-10:])
    # Naive Bayes Classification
    nb = MultinomialNB()
    nb.fit(X_train, y_train)
    y_pred = nb.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\nAccuracy: {accuracy:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)







